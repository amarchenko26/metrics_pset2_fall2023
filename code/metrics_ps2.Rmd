---
title: "Applied Metrics Empirical P-Set 2, Fall 2023"
output: pdf_document
date: "2023-11-29"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, tidy.opts=list(width.cutoff=60), tidy=TRUE)
```

### Load data

```{r }
# Clear all
rm(list = ls())

# load and require packages
pacman::p_load(foreign, tidyverse, here, randomForest, boot, MASS, ivmte)

# Load data
data <- read.csv(here("data/andres_cleaned.csv"))
data$X <- NULL

set.seed(1234) # For reproducibility


```

### Question 1a - Relate structural equation and probit regression

Suppose we specify the potential outcome equations as $Y_i(1) = X'_i \theta_1 + \epsilon_{1i}$ and $Y_i(0) = X'_i \theta_0 + \epsilon_{0i}$, and the structural selection equation as $D_i = 1 \{u(X_i,Z_i) \geq V_i \}$. where $u(X_i,Z_i) - V_i$ is the latent utility derived from additional children.

The structural selection equation describes the switch from $D = 0$ to $D=1$ in terms of utility, i.e., whether, as a function of your observables and instrument, the family has a higher utility than some level of having more than three children. The propensity score quantifies this probability, capturing the likelihood, given your observables and instrument value, of having $D = 1$.

### Question 1b - Assumptions for MTE

Clarify a set of assumptions that the marginal treatment effect $M(x,u)$, $u \in (0,1)$ can be identified by the derivative of the OLS regression equation WRT the propensity score: $$MTE(x, u) = \frac{\partial}{\partial p}\bigg|_{p=u} \left( x'\beta_0 + px'\beta_1 + \kappa_1p + \kappa_2p^2 + \kappa_3p^3 \right)$$

The assumptions are: XXXXX

### Question 1c - Estimate propensity score

Estimate the propensity score as instructed above, and assess whether the coefficient of the instrumental variable in the probit regression is significantly different from zero or not.

Yes, we see that the z-value for the instrument is extremely large at 37, and very significant. So we have confidence that the instrument is predicting treatment.

```{r }
# Fit probit
probit_model <- glm(d ~ blackm + hispm + othracem + agem1 + agefstm + boy1st + z, 
                    data = data, 
                    family = binomial(link = "probit"))

# Predict propensity scores
data$prop_score <- predict(probit_model, type = "response")

summary(probit_model)
```

### Question 1d - Plot histogram of estimated propensity scores

```{r }
hist(data$prop_score, 
     main = "Histogram of Estimated Propensity Scores", 
     xlab = "Propensity Score", 
     ylab = "Frequency",
     col = "blue",
     border = "black")
```

```{r }
summary(probit_model)
```

### Question 1e - Estimate by OLS, bootstrap standard errors

Report the OLS estimates of the coefficients in the regression equation of (2) and their bootstrap standard errors with 1000 bootstrap replications.

```{r }
# Fit OLS
ols <- lm(y ~ blackm + hispm + othracem + agem1 + agefstm + boy1st + 
            prop_score*(blackm + hispm + othracem + agem1 + agefstm + boy1st) + 
            prop_score + I(prop_score^2) + I(prop_score^3), 
                    data = data)

summary(ols)

# Function to fit model on bootstrap sample
boot_fn <- function(data, index) {
  fit <- lm(y ~ blackm + hispm + othracem + agem1 + agefstm + boy1st + 
            prop_score*(blackm + hispm + othracem + agem1 + agefstm + boy1st) + 
            prop_score + I(prop_score^2) + I(prop_score^3), 
            data = data, subset = index)
  return(coef(fit))
}

# Perform bootstrap
#boot_results <- boot(data, boot_fn, R = 1000)

# Calculate standard errors
#boot_se <- apply(boot_results$t, 2, sd)
```

### Question 1f - Calculate MTE and their CI

Plot the MTE estimates (as a function of u âˆˆ [0,1]) and pointwise confidence intervals, holding X constant at its sample mean values. The confidence intervals are constructed using the boostrap results.

```{r}

# Step 1: Calculate the means of the covariates
covariate_means <- colMeans(data[, c("blackm", "hispm", "othracem", "agem1", "agefstm", "boy1st")], na.rm = TRUE)

# Calculate the first term of the derivative (x prime times beta1)
beta1 <- coef(ols)[11:16]
xprime <- matrix(covariate_means, nrow = 1, ncol = 6) #transpose to make 1x6
beta1 <- matrix(beta1, nrow = 6, ncol = 1) #transpose to make 6x1
xbeta1 <- xprime %*% beta1

kappa1 <- coef(ols)[8] #prop_score coef
kappa2 <- coef(ols)[9] #prop_score^2 coef
kappa3 <- coef(ols)[10] #prop_score^3 coef

# define u 
u <- seq(0, 1, length.out = 100)

marginal_effects <- as.vector(xbeta1) + as.vector(kappa1) + (2 * as.vector(kappa2) * u) + (3 * as.vector(kappa3) * (u)^2)

# Create a new data frame for plotting
plot_data <- data.frame(xvar = u, marginal_effects = marginal_effects)

# Use ggplot to plot MTEs
plot <- ggplot(plot_data, aes(x = u, y = marginal_effects)) +
  geom_point() +
  theme_minimal() +
  labs(x = "u", y = "Marginal Effects", 
       title = "Scatterplot of Marginal Effects vs. Unobserved Heterogeneity u")

kappa_1_boot <- boot_results$t[,8]
kappa_2_boot <- boot_results$t[,9]
kappa_3_boot <- boot_results$t[,10]
beta_1_boot <- boot_results$t[,11:16]
xbeta1_boot <- xprime %*% t(beta_1_boot)

marginal_effects_matrix <- as.vector(xbeta1_boot) + as.vector(kappa_1_boot) + (2 * u %*% t(kappa_2_boot)) + (3 * u^2 %*% t(kappa_3_boot))

MTE_lb <- apply(marginal_effects_matrix, 1, function(x) quantile(x, probs = 0.025))
MTE_ub <- apply(marginal_effects_matrix, 1, function(x) quantile(x, probs = 0.975))

# Create a data frame for ggplot
plot_data <- data.frame(u, marginal_effects, MTE_lb, MTE_ub)

# Plot using ggplot
mte_plot <- ggplot(plot_data, aes(x = u, y = marginal_effects)) +
  geom_line() +
  geom_ribbon(aes(ymin = MTE_lb, ymax = MTE_ub), alpha = 0.2) +
  labs(x = "u", y = "Marginal Effects", title="MTE vs. u with confidence intervals") +
  theme_minimal()

mte_plot

```

### Question 1g - Interpret

Based on the presented estimates, interpret heterogeneity of the marginal treatment effects. What type of parent has a larger causal effect than others? In your answer, be explicit about how to interpret u.

In this case, the marginal treatment effect is the difference betewen the marginal treatment response functions of treated and untreated individuals, conditional on \$U = u\$ and \$X_i = x\$. $u$ is the unobservable latent variable in the selection equation. That is, \$u\$ captures the unobservable propensity to take up treatment. The monotonicity assumption allows us to order individuals in a one-dimensional manner according to their latent propensity to take up treatment.

If MTE is declining in \$u\$, that means that people who are less likely to choose treatment experience smaller treatment effects than those who are more likely to choose treatment.

QUESTION - is it that $u$ is the unobservable heterogeneity ehre?

## Question 2

### Question 2a

```{r}
# Worked with Raquel Badillo-Salas

set.seed(1234)

# Set the number of variables and observations
n_vars <- 20
n_obs <- 500

# Create the variance matrix sigma
sigma <- matrix(0, nrow = n_vars, ncol = n_vars)
for (i in 1:n_vars) {
  for (j in 1:n_vars) {
    sigma[i, j] <- 0.8^abs(j - i)
  }
}

# Ensure diagonal entries are 1
diag(sigma) <- 1

# Generate the data
data2 <- as.data.frame(mvrnorm(n = n_obs, mu = rep(0, n_vars), Sigma = sigma)) # Need MASS package

###################################

# Set up
a0 <- 1
a1 <- 0.25
b0 <- 1
b1 <- 0.25
tau <- 0.5

# Define m and V
m <- function(X1, X3){
    return(a0*X1 + a1*(exp(X3)/(1+exp(X3))))
}

V_fun <- function(X1, X3){
  zeta <- rnorm(500,0,1)
  return(m(X1, X3) + zeta)
}

V <- V_fun(data2$V1, data2$V3)
data2$D <- ifelse(V > 0, 1, 0)

# Define g(X,D) and Y
g_fun <- function(D, X1, X3){
    return(tau*D + b0*(exp(X1)/(1+exp(X1))) + b1*X3)
}

Y_fun <- function(X1, X3, D){
  epsilon <- rnorm(500, 0, 1)
  return(g(D, X1, X3) + epsilon)
}

data2$Y <- Y_fun(data2$V1, data2$V3, data2$D)

###################################

independent_vars <- paste0("V", 1:20, collapse = " + ")
formula <- as.formula(paste("D ~ ", independent_vars))

# Fit probit
probit_model <- glm(formula, 
                    data = data2, 
                    family = binomial(link = "probit"))

# Predict propensity scores
data2$prop_score <- predict(probit_model, type = "response")

summary(probit_model)

data2$w_ate <- (data2$D * data2$Y/(data2[["prop_score"]])) - ((1-data2$D) * data2$Y/(1-data2[["prop_score"]]))

ate_probit <- mean(data2$w_ate)
```

### Question 2b - Estimate ATE using Random Forest.

```{r}
# Fit Random Forest models
# Replace 'Y ~ .' with 'Y ~ [covariates]' to specify covariates
rf_treatment <- randomForest(Y ~ ., data = data2[data2$D == 1, ])
rf_control <- randomForest(Y ~ ., data = data2[data2$D == 0, ])

# Predict outcomes using the fitted models
rf_pred_treatment <- predict(rf_treatment, treatment_data)
rf_pred_control <- predict(rf_control, control_data)

# Estimate Average Treatment Effect (ATE)
ate_rf <- mean(rf_pred_treatment) - mean(rf_pred_control)

# Output the ATE
ate_rf

```

### Question 2c - Estimate ATE using doubly robust estimator

Estimate the ATE using the doubly robust estimator: $$
\theta_{\text{ATE}_{\text{DR}}} = \mathbb{E} \left[ 
\frac{(Y - g(X, 1))D}{p(X)} - 
\frac{(Y - g(X, 0))(1 - D)}{1 - p(X)} + 
\left( g(X, 1) - g(X, 0) \right) 
\right]
$$ where the nuisance functions $p(X) = \mathbb{E}[D|X]$ and $g(X, D) = \mathbb{E}[Y|X, D]$ are estimated using random forest as well.


```{r}

rf_data <- subset(data2, select = c(-Y, -w_ate, -prop_score))

# Use RF to estimate p(X) 
rf_prop <- randomForest(D ~ ., data = rf_data)
rf_data$rf_prop_pred <- predict(rf_prop, rf_data)

# Append other vars
g1 <- rf_pred_treatment
g0 <- rf_pred_control
rf_data$Y <- data2$Y

# Trim the prop scores
rf_data <- rf_data %>% filter(rf_prop_pred > 0.01 & rf_prop_pred < 0.99)

# Calculate the doubly robust estimator for the ATE
doubly_robust_ate <-  mean(((rf_data$Y[D==1] - g1) * rf_data$D[D==1] / rf_data$rf_prop_pred[D==1]) - ((rf_data$Y[D==0] - g0) * rf_data$D[D==0] / rf_data$rf_prop_pred[D==0]) +  (g1 - g0))

# Output the ATE
doubly_robust_ate

```





### Question 2d -

```{r}


```
