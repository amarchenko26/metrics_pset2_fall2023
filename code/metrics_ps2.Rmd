---
title: "Applied Metrics Empirical P-Set 2, Fall 2023"
output: pdf_document
date: "2023-11-29"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, tidy.opts=list(width.cutoff=60), tidy=TRUE)
```

### Load data

```{r }
# Clear all
rm(list = ls())

# load and require packages
pacman::p_load(foreign, tidyverse, here, randomForest, boot, MASS, ivmte)

# Load data
data <- read.csv(here("data/andres_cleaned.csv"))
data$X <- NULL

set.seed(1234) # For reproducibility


```

### Question 1a - Relate structural equation and probit regression

Suppose we specify the potential outcome equations as $Y_i(1) = X'_i \theta_1 + \epsilon_{1i}$ and $Y_i(0) = X'_i \theta_0 + \epsilon_{0i}$, and the structural selection equation as $D_i = 1 \{u(X_i,Z_i) \geq V_i \}$. where $u(X_i,Z_i) - V_i$ is the latent utility derived from additional children.

The structural selection equation describes the switch from $D = 0$ to $D=1$ in terms of utility, i.e., whether, as a function of your observables and instrument, the family has a higher utility than some level of having more than three children. The propensity score quantifies this probability, capturing the likelihood, given your observables and instrument value, of having $D = 1$.

### Question 1b - Assumptions for MTE

Clarify a set of assumptions that the marginal treatment effect $M(x,u)$, $u \in (0,1)$ can be identified by the derivative of the OLS regression equation WRT the propensity score: $$MTE(x, u) = \frac{\partial}{\partial p}\bigg|_{p=u} \left( x'\beta_0 + px'\beta_1 + \kappa_1p + \kappa_2p^2 + \kappa_3p^3 \right)$$

The assumptions are: XXXXX

### Question 1c - Estimate propensity score

Estimate the propensity score as instructed above, and assess whether the coefficient of the instrumental variable in the probit regression is significantly different from zero or not.

Yes, we see that the z-value for the instrument is extremely large at 37, and very significant. So we have confidence that the instrument is predicting treatment.

```{r }
# Fit probit
probit_model <- glm(d ~ blackm + hispm + othracem + agem1 + agefstm + boy1st + z, 
                    data = data, 
                    family = binomial(link = "probit"))

# Predict propensity scores
data$prop_score <- predict(probit_model, type = "response")

summary(probit_model)
```

### Question 1d - Plot histogram of estimated propensity scores

```{r }
hist(data$prop_score, 
     main = "Histogram of Estimated Propensity Scores", 
     xlab = "Propensity Score", 
     ylab = "Frequency",
     col = "blue",
     border = "black")
```

```{r }
summary(probit_model)
```

### Question 1e - Estimate by OLS, bootstrap standard errors

Report the OLS estimates of the coefficients in the regression equation of (2) and their bootstrap standard errors with 1000 bootstrap replications.

```{r }
# Fit OLS
ols <- lm(y ~ blackm + hispm + othracem + agem1 + agefstm + boy1st + 
            prop_score*(blackm + hispm + othracem + agem1 + agefstm + boy1st) + 
            prop_score + I(prop_score^2) + I(prop_score^3), 
                    data = data)

summary(ols)

# Function to fit model on bootstrap sample
boot_fn <- function(data, index) {
  fit <- lm(y ~ blackm + hispm + othracem + agem1 + agefstm + boy1st + 
            prop_score*(blackm + hispm + othracem + agem1 + agefstm + boy1st) + 
            prop_score + I(prop_score^2) + I(prop_score^3), 
            data = data, subset = index)
  return(coef(fit))
}

# Perform bootstrap
#boot_results <- boot(data, boot_fn, R = 1000)

# Calculate standard errors
#boot_se <- apply(boot_results$t, 2, sd)
```

### Question 1f - Calculate MTE and their CI

Plot the MTE estimates (as a function of u âˆˆ [0,1]) and pointwise confidence intervals, holding X constant at its sample mean values. The confidence intervals are constructed using the boostrap results.

```{r}

# Step 1: Calculate the means of the covariates
covariate_means <- colMeans(data[, c("blackm", "hispm", "othracem", "agem1", "agefstm", "boy1st")], na.rm = TRUE)

# Calculate the first term of the derivative (x prime times beta1)
beta1 <- coef(ols)[11:16]
xprime <- matrix(covariate_means, nrow = 1, ncol = 6) #transpose to make 1x6
beta1 <- matrix(beta1, nrow = 6, ncol = 1) #transpose to make 6x1
xbeta1 <- xprime %*% beta1

kappa1 <- coef(ols)[8] #prop_score coef
kappa2 <- coef(ols)[9] #prop_score^2 coef
kappa3 <- coef(ols)[10] #prop_score^3 coef

# define u 
u <- seq(0, 1, length.out = 100)

marginal_effects <- as.vector(xbeta1) + as.vector(kappa1) + (2 * as.vector(kappa2) * u) + (3 * as.vector(kappa3) * (u)^2)

# Create a new data frame for plotting
plot_data <- data.frame(xvar = u, marginal_effects = marginal_effects)

# Use ggplot to plot MTEs
plot <- ggplot(plot_data, aes(x = u, y = marginal_effects)) +
  geom_point() +
  theme_minimal() +
  labs(x = "u", y = "Marginal Effects", 
       title = "Scatterplot of Marginal Effects vs. Unobserved Heterogeneity u")

kappa_1_boot <- boot_results$t[,8]
kappa_2_boot <- boot_results$t[,9]
kappa_3_boot <- boot_results$t[,10]
beta_1_boot <- boot_results$t[,11:16]
xbeta1_boot <- xprime %*% t(beta_1_boot)

marginal_effects_matrix <- as.vector(xbeta1_boot) + as.vector(kappa_1_boot) + (2 * u %*% t(kappa_2_boot)) + (3 * u^2 %*% t(kappa_3_boot))

MTE_lb <- apply(marginal_effects_matrix, 1, function(x) quantile(x, probs = 0.025))
MTE_ub <- apply(marginal_effects_matrix, 1, function(x) quantile(x, probs = 0.975))

# Create a data frame for ggplot
plot_data <- data.frame(u, marginal_effects, MTE_lb, MTE_ub)

# Plot using ggplot
mte_plot <- ggplot(plot_data, aes(x = u, y = marginal_effects)) +
  geom_line() +
  geom_ribbon(aes(ymin = MTE_lb, ymax = MTE_ub), alpha = 0.2) +
  labs(x = "u", y = "Marginal Effects", title="MTE vs. u with confidence intervals") +
  theme_minimal()

mte_plot

```

### Question 1g - Interpret

Based on the presented estimates, interpret heterogeneity of the marginal treatment effects. What type of parent has a larger causal effect than others? In your answer, be explicit about how to interpret u.

In this case, the marginal treatment effect is the difference betewen the marginal treatment response functions of treated and untreated individuals, conditional on \$U = u\$ and \$X_i = x\$. $u$ is the unobservable latent variable in the selection equation. That is, \$u\$ captures the unobservable propensity to take up treatment. The monotonicity assumption allows us to order individuals in a one-dimensional manner according to their latent propensity to take up treatment.

If MTE is declining in \$u\$, that means that people who are less likely to choose treatment experience smaller treatment effects than those who are more likely to choose treatment.

QUESTION - is it that $u$ is the unobservable heterogeneity ehre?

## Question 2

### Question 2a

```{r}
# Worked with Raquel Badillo-Salas

set.seed(1234)

# Set the number of variables and observations
n_vars <- 20
n_obs <- 500

# Create the variance matrix sigma
sigma <- matrix(0, nrow = n_vars, ncol = n_vars)
for (i in 1:n_vars) {
  for (j in 1:n_vars) {
    sigma[i, j] <- 0.8^abs(j - i)
  }
}

# Ensure diagonal entries are 1
diag(sigma) <- 1

# Generate the data
data2 <- as.data.frame(mvrnorm(n = n_obs, mu = rep(0, n_vars), Sigma = sigma)) # Need MASS package

###################################

# Set up
a0 <- 1
a1 <- 0.25
b0 <- 1
b1 <- 0.25
tau <- 0.5

# Define m and V
m <- function(X1, X3){
    return(a0*X1 + a1*(exp(X3)/(1+exp(X3))))
}

V_fun <- function(X1, X3){
  zeta <- rnorm(500,0,1)
  return(m(X1, X3) + zeta)
}

V <- V_fun(data2$V1, data2$V3)
data2$D <- ifelse(V > 0, 1, 0)

# Define g(X,D) and Y
g_fun <- function(D, X1, X3){
    return(tau*D + b0*(exp(X1)/(1+exp(X1))) + b1*X3)
}

Y_fun <- function(X1, X3, D){
  epsilon <- rnorm(500, 0, 1)
  return(g(D, X1, X3) + epsilon)
}

data2$Y <- Y_fun(data2$V1, data2$V3, data2$D)

###################################

independent_vars <- paste0("V", 1:20, collapse = " + ")
formula <- as.formula(paste("D ~ ", independent_vars))

# Fit probit
probit_model <- glm(formula, 
                    data = data2, 
                    family = binomial(link = "probit"))

# Predict propensity scores
data2$prop_score <- predict(probit_model, type = "response")

summary(probit_model)

data2$w_ate <- (data2$D * data2$Y/(data2[["prop_score"]])) - ((1-data2$D) * data2$Y/(1-data2[["prop_score"]]))

ate_probit <- mean(data2$w_ate)
```

### Question 2b - Estimate ATE using Random Forest.

```{r}
# Fit Random Forest models
# Replace 'Y ~ .' with 'Y ~ [covariates]' to specify covariates
rf_treatment <- randomForest(Y ~ ., data = data2[data2$D == 1, ])
rf_control <- randomForest(Y ~ ., data = data2[data2$D == 0, ])

# Predict outcomes using the fitted models
rf_pred_treatment <- predict(rf_treatment, treatment_data)
rf_pred_control <- predict(rf_control, control_data)

# Estimate Average Treatment Effect (ATE)
ate_rf <- mean(rf_pred_treatment) - mean(rf_pred_control)

# Output the ATE
ate_rf

```

### Question 2c - Estimate ATE using doubly robust estimator

Estimate the ATE using the doubly robust estimator: $$
\theta_{\text{ATE}_{\text{DR}}} = \mathbb{E} \left[ 
\frac{(Y - g(X, 1))D}{p(X)} - 
\frac{(Y - g(X, 0))(1 - D)}{1 - p(X)} + 
\left( g(X, 1) - g(X, 0) \right) 
\right]
$$ where the nuisance functions $p(X) = \mathbb{E}[D|X]$ and $g(X, D) = \mathbb{E}[Y|X, D]$ are estimated using random forest as well.


```{r}

rf_data <- subset(data2, select = c(-Y, -w_ate, -prop_score))

# Use RF to estimate p(X) 
rf_prop <- randomForest(D ~ ., data = rf_data)
rf_data$rf_prop_pred <- predict(rf_prop, rf_data)

# Append other vars
g1 <- rf_pred_treatment
g0 <- rf_pred_control
rf_data$Y <- data2$Y

# Trim the prop scores
rf_data <- rf_data %>% filter(rf_prop_pred > 0.01 & rf_prop_pred < 0.99)

# Calculate the doubly robust estimator for the ATE
doubly_robust_ate <-  mean(((rf_data$Y[D==1] - g1) * rf_data$D[D==1] / rf_data$rf_prop_pred[D==1]) - ((rf_data$Y[D==0] - g0) * rf_data$D[D==0] / rf_data$rf_prop_pred[D==0]) +  (g1 - g0))

# Output the ATE
doubly_robust_ate

```



### Question 2d - Estime ATE using cross-fitting

Estimate the ATE using the doubly robust estimator as before but compute the first step using cross-fitting for I = 2 sample split in the following way.

```{r}

# Subset the data
data_2d <- subset(data2, select = -c(w_ate, prop_score))

# Divide sample into two folds
set.seed(123)  # Set seed for reproducibility
n <- nrow(data_2d)
id_div1 <- sample(1:n, n/2)
id_div2 <- setdiff(1:n, id_div1)
data_2d_DD_1 <- data_2d[id_div1, ]
data_2d_DD_2 <- data_2d[id_div2, ]

# Probit model for propensity score estimation
ps_estimator <- function(df) {
  probit_model <- glm(D ~ ., data = df, family = binomial(link = "probit"))
  df$ps_pred <- predict(probit_model, type = "response")
  df$ate_probit <- (df$D * df$Y / df$ps_pred) - ((1 - df$D) * df$Y / (1 - df$ps_pred))
  return(df)
}

# Naive estimation using random forests
theta_naive_fun <- function(df) {
  treatment <- subset(df, D == 1, select = -c(D, ps_pred, ate_probit))
  control <- subset(df, D == 0, select = -c(D, ps_pred, ate_probit))
  
  rf_treatment <- randomForest(Y ~ ., data = treatment)
  rf_control <- randomForest(Y ~ ., data = control)
  
  df$g0 <- predict(rf_control, df)
  df$g1 <- predict(rf_treatment, df)

  ate_naive <- mean(df$g1 - df$g0)
  return(df)
}

# Double Robustness (DR) estimation with trimming
theta_DR_trim <- function(df) {
  trimmed_df <- df %>% 
    filter(ps_pred > 0.01, ps_pred < 0.99) %>% 
    mutate(left = (Y - g1) * D / ps_pred - (Y - g0) * (1 - D) / (1 - ps_pred),
           right = g1 - g0)
  return(trimmed_df)
}

# Combined estimation function
theta_DD <- function(df) {
  df <- ps_estimator(df)
  ate_probit <- mean(df$ate_probit)
  
  df <- theta_naive_fun(df)
  ate_naive <- mean(df$g1 - df$g0)

  df <- theta_DR_trim(df)
  ate_DR <- mean(df$left + df$right)

  return(c(ate_probit, ate_naive, ate_DR))
}

# Apply estimation on the full data and folds
eta_j <- theta_DD(data_2d)
eta_1 <- theta_DD(data_2d_DD_1)
eta_2 <- theta_DD(data_2d_DD_2)

# Output results
list(eta_j, eta_1, eta_2)

# Estimate ATE by DD (using DR estimations in each fold)
ate_DD <- (eta_1[3] + eta_2[3]) / 2
ate_DD

```

### Question 2e - Repeat process

Repeat process for 1000 Montecarlo simulations and compute the four estimators for each sample.

```{r}

# Generate the data
n_iter <- 1000

# Store bootstrap results
ate_list <- vector("list", length = n_iter)

# Iterating over bootstrap samples
for (i in 1:n_iter) {
  # Generate data for each iteration
  data_ex2_iter <- as.data.frame(MASS::mvrnorm(n = 500, mu = rep(0, 20), Sigma = sigma)) 
  V <- V_fun(data_ex2_iter$V1, data_ex2_iter$V3)
  data_ex2_iter$D <- ifelse(V > 0, 1, 0)
  data_ex2_iter$Y <- Y_fun(data_ex2_iter$V1, data_ex2_iter$V3, data_ex2_iter$D)

  # Divide data into two groups
  id_div1_iter <- sample(1:length(data_ex2_iter$Y), length(data_ex2_iter$Y) / 2, replace = FALSE)
  id_div2_iter <- setdiff(1:length(data_ex2_iter$Y), id_div1_iter)
  data_ex2_DD_1_iter <- data_2d[id_div1_iter, ]
  data_ex2_DD_2_iter <- data_2d[id_div2_iter, ]

  # Calculate coefficients
  eta_j_iter <- theta_DD(data_ex2_iter)
  eta_1_iter <- theta_DD(data_ex2_DD_1_iter)
  eta_2_iter <- theta_DD(data_ex2_DD_2_iter)   
  ate_DD_iter <- (eta_1_iter[3] + eta_2_iter[3]) / 2

  # Append coefficients to the list
  all_coef <- append(eta_j_iter, ate_DD_iter)
  ate_list[[i]] <- all_coef
}

# Optionally save the list
# saveRDS(ate_list, here("/ate_list.rds"))

# Return list
ate_list

```


### Question 2f - Compute empirical variance of each estimator

Compute the empirical variance of each of your estimators $\hat{\sigma}^2_{\theta_k}$ where k = probit, naive, DR, DD. 

```{r}
ate_matrix <- do.call(rbind, ate_list)
variances <- apply(ate_matrix, 2, var)
variances

```  


### Question 2g - Plots

Create four plots, one for each estimator, containing the histogram of the $\hat{\theta}_{ATE_k}$ and a Normal(.5, $\hat{\sigma}^2_{\theta_k}$) pdf.

```{r}
df_ate <- as.data.frame(ate_matrix)
colnames(df_ate) <- c("ate_probit", "ate_naive", "ate_DR", "ate_DD")

# Define the names of the columns in df_ate
ate_names <- c("ate_probit", "ate_naive", "ate_DR", "ate_DD")

# Loop through each column name to create the plots
plots <- lapply(1:length(ate_names), function(i) {
  ggplot(df_ate, aes_string(x = ate_names[i])) +
    geom_density(fill = "purple", alpha = 0.5) +
    stat_function(fun = dnorm, args = list(mean = 0.5, sd = sqrt(variances[i])), color = "blue", linewidth = 1) +
    geom_vline(xintercept = 0.5, linetype = "dashed", color = "blue") +
    labs(title = paste(ate_names[i], "density and Normal Distribution"),
         x = "Values",
         y = "Density") +
    theme_minimal()
})

for (plot in plots) print(plot)

``` 

### Question 2h - Table

Present a table comparing the bias and mean square error of each estimator and comment
on which estimator performs better and why.

```{r}

# Function to calculate MSE
calculate_mse <- function(n, mu, Sigma, method_ate) {
  observed <- as.data.frame(MASS::mvrnorm(n = n, mu = mu, Sigma = Sigma))
  return(mean((observed$V1 - method_ate)^2))
}

# Calculate bias
bias <- c(0.5, 0.5, 0.5, 0.5) - apply(ate_matrix, 2, mean)

# Initialize method names and corresponding ATEs
methods <- c("Probit", "Naive", "DR", "DD")
ates <- c(df_ate$ate_probit, df_ate$ate_naive, df_ate$ate_DR, df_ate$ate_DD)

# Calculate MSE for each method
mse_values <- sapply(1:4, function(i) calculate_mse(1000, 0.5, variances[i], ates[i]))

# Create a data frame
data <- data.frame(
  Method = methods,
  Bias = round(bias, 4), 
  MSE = round(mse_values, 4)
)

# Print the formatted table
knitr::kable(data, format = "markdown", caption = "Bias and MSE for Different Methods")

``` 














